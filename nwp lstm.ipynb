{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "corpus = \"\"\"\n",
    "Deep learning allows machines to learn from data.\n",
    "It helps in predicting future outcomes.\n",
    "Deep learning models are powerful and flexible.\n",
    "Learning from examples is the key to generalization.\n",
    "Models trained well can predict future words.\n",
    "The model learns sequence patterns and predicts the next token.\n",
    "\"\"\"\n",
    "\n",
    "corpus = corpus.lower().replace('\\n', ' ').strip()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus])\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([corpus])[0]\n",
    "sequences = []\n",
    "for i in range(1, len(encoded)):\n",
    "    seq = encoded[: i+1]\n",
    "    sequences.append(seq)\n",
    "\n",
    "max_len = max(len(s) for s in sequences)\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "sequences_padded = np.array(sequences_padded)\n",
    "X = sequences_padded[:, :-1]\n",
    "y = sequences_padded[:, -1]\n",
    "y_cat = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "embed_dim = 50\n",
    "lstm_units = 100\n",
    "input_length = X.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=input_length),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y_cat, epochs=120, batch_size=16, verbose=0)\n",
    "\n",
    "def predict_next_word(model, tokenizer, text, max_input_len):\n",
    "    text_proc = text.lower().strip()\n",
    "    encoded = tokenizer.texts_to_sequences([text_proc])[0]\n",
    "    if len(encoded) == 0:\n",
    "        return \"<unknown>\"\n",
    "    encoded_padded = pad_sequences([encoded], maxlen=max_input_len, padding='pre')\n",
    "    preds = model.predict(encoded_padded, verbose=0)\n",
    "    pred_index = int(np.argmax(preds, axis=-1)[0])\n",
    "    return index_word.get(pred_index, \"<unknown>\")\n",
    "\n",
    "examples = [\"deep learning\", \"learning models\", \"predict future\"]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for ex in examples:\n",
    "    next_w = predict_next_word(model, tokenizer, ex, input_length)\n",
    "    print(f\"Input: '{ex}' -> Next word: '{next_w}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
